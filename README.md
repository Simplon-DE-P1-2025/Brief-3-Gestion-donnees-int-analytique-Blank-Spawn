# Brief-3-Gestion-donnees-int-analytique-Blank-Spawn
Gestion centralisÃ©e de donnÃ©es et interface analytique interactive.

## ğŸ“‹ Contexte
Projet de centralisation et visualisation des donnÃ©es pour le dÃ©partement de surveillance et sauvetage. Le systÃ¨me intÃ¨gre un pipeline ETL robuste, une validation de donnÃ©es stricte et une interface utilisateur complÃ¨te avec CRUD et dashboards analytiques.

## ğŸ¯ Objectifs
- âœ… RÃ©cupÃ©rer et nettoyer les donnÃ©es
- âœ… Centraliser dans une base de donnÃ©es (PostgreSQL)
- âœ… CrÃ©er une interface CRUD et des dashboards analytiques (Streamlit)
- âœ… Valider les donnÃ©es (Pandera)
- âœ… Documenter et tester (Pytest)
- âœ… Orchestrer avec Docker/Docker Compose

## ğŸ› ï¸ Stack technique
- **Backend**: Python 3.9+
- **Validation**: Pandera
- **Interface**: Streamlit
- **Base de donnÃ©es**: PostgreSQL
- **Orchestration**: Docker & Docker Compose
- **Tests**: Pytest
- **Infrastructure**: Supabase (optionnel)

## ğŸ“ Structure du projet

```
â”œâ”€â”€ pipeline/                    # Pipeline ETL
â”‚   â”œâ”€â”€ main.py                 # Point d'entrÃ©e du pipeline
â”‚   â”œâ”€â”€ schemas/                # SchÃ©mas de validation Pandera
â”‚   â”œâ”€â”€ utils/                  # Utilitaires (BD, types de donnÃ©es)
â”‚   â””â”€â”€ data/                   # DonnÃ©es d'entrÃ©e/sortie
â”œâ”€â”€ streamlit_app/              # Application Streamlit
â”‚   â”œâ”€â”€ 1_Home.py              # Page d'accueil
â”‚   â”œâ”€â”€ pages/                 # Pages additionnelles (Dashboard, CRUD, Audit, Logs)
â”‚   â”œâ”€â”€ utils/                 # Utilitaires (authentification, BDD, helpers)
â”‚   â”œâ”€â”€ data_loader.py         # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ utils.py               # Fonctions utilitaires
â”‚   â””â”€â”€ visualizations.py      # Composants de visualisation
â”œâ”€â”€ support_tools/              # Scripts de dÃ©veloppement
â”‚   â”œâ”€â”€ sql_scripts/           # Scripts SQL (tables, KPI, analyse)
â”‚   â””â”€â”€ *.py                   # Scripts d'import et nettoyage
â”œâ”€â”€ tests/                      # Tests unitaires (Pytest)
â”œâ”€â”€ docker-compose.yml          # Configuration Docker
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â””â”€â”€ README.md                   # Cette documentation

```

### Descriptions des rÃ©pertoires

- **pipeline** - Pipeline ETL complet pour extraction, transformation et chargement des donnÃ©es
- **streamlit_app** - Application Streamlit avec interface utilisateur complÃ¨te (accueil, dashboard, CRUD, audit, logs)
- **support_tools** - Scripts utilitaires pour dÃ©veloppement et gestion des donnÃ©es
- **tests** - Suite de tests unitaires

## ğŸš€ DÃ©marrage rapide

### 1. PrÃ©requis
- Python 3.9 ou supÃ©rieur
- Docker et Docker Compose (optionnel)
- Git

### 2. Installation locale

#### Cloner le projet et installer les dÃ©pendances
```bash
# CrÃ©er et activer l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate  # Sur Windows: .venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

#### Configurer les variables d'environnement
CrÃ©ez un fichier `.env` Ã  la racine du projet avec les variables suivantes :

```env
# Supabase (si utilisÃ©)
SUPABASE_URL=votre_url_supabase
SUPABASE_KEY=votre_clÃ©_supabase

# Base de donnÃ©es PostgreSQL
user=postgres
password=votre_mot_de_passe
host=localhost
port=5432
dbname=nom_de_la_base
```

### 3. Lancer le pipeline ETL
```bash
python3 pipeline/main.py
```

### 4. Lancer l'application Streamlit
```bash
streamlit run streamlit_app/1_Home.py
```

L'application sera accessible sur `http://localhost:8501`

## ğŸ³ Utilisation avec Docker Compose

Pour une mise en place complÃ¨te avec PostgreSQL en conteneur :

### DÃ©marrer la base de donnÃ©es
```bash
docker-compose up -d
```

Cette commande lance un conteneur PostgreSQL avec les configurations dÃ©finies dans le fichier `.env`.

### VÃ©rifier le statut
```bash
docker-compose ps
```

### ArrÃªter la base de donnÃ©es
```bash
docker-compose down
```

### ArrÃªter et supprimer tous les volumes
```bash
docker-compose down -v
```

## ğŸš€ Lancement du projet en local

### Installation des dÃ©pendances
Avant de lancer les tests, assurez-vous d'installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

### Activation de l'environnement virtuel
Activez l'environnement virtuel Python :
```bash
source .venv/bin/activate
```

### Fichiers d'environnement
Le projet utilisant une base de donnÃ©es hÃ©bergÃ©e en ligne, nous utilisons des fichiers d'environnement pour stocker des variables nÃ©cessaires Ã  la connexion en BDD.
Un premier fichier `.env` Ã  placer Ã  la racine du projet:

Les donnÃ©es nÃ©cessaires dans ces fichiers
- SUPABASE_URL= `URL de connexion Ã  Supabase`
- SUPABASE_KEY= `clÃ© Supabase`
- user= `utilisateur de base de donnÃ©es`
- password= `mot de passe utilisateur`
- host= `IP/Nom d'hÃ´te pour la connexion Ã  la base`
- port= `port de connexion`
- dbname= `nom de la base`

### Lancement
```bash
python3 pipeline/main.py
```

## ğŸ³ Lancement du projet avec Docker Compose

Pour utiliser Docker Compose afin de lancer la base de donnÃ©es PostgreSQL localement :

### PrÃ©requis
- Docker et Docker Compose installÃ©s sur votre machine.
- Fichier `.env` configurÃ© avec les variables d'environnement nÃ©cessaires (voir section "Fichiers d'environnement").

### DÃ©marrage de la base de donnÃ©es
```bash
docker-compose up -d
```
Cette commande lance le conteneur PostgreSQL en arriÃ¨re-plan. Le service sera accessible sur le port dÃ©fini dans la variable `POSTGRES_PORT` du fichier `.env`.

### ArrÃªt de la base de donnÃ©es
```bash
docker-compose down
```
Cela arrÃªte et supprime les conteneurs lancÃ©s par Docker Compose.

AprÃ¨s avoir dÃ©marrÃ© la base de donnÃ©es avec Docker Compose, vous pouvez lancer le pipeline comme indiquÃ© dans la section "Lancement du projet en local".

## ğŸ“Š AccÃ¨s Ã  l'application

### Pages disponibles
1. **1_Home.py** - Page d'accueil et prÃ©sentation
2. **2_Dashboard.py** - Dashboards analytiques
3. **3_Crud.py** - Gestion complÃ¨te des donnÃ©es (Create, Read, Update, Delete)
4. **4_Audit.py** - Audit trail et historique des modifications
5. **5_Logs.py** - Logs du systÃ¨me et debugging

### Authentification
L'application inclut un systÃ¨me d'authentification basique. Consultez `streamlit_app/utils/auth_ui.py` pour les dÃ©tails.

## ğŸ—„ï¸ Architecture de la base de donnÃ©es

La structure de la base est documentÃ©e dans le schÃ©ma :
<div align="center">
  <img src="assets/schema.png" alt="schema BDD" width="600">
</div>

Les scripts SQL de crÃ©ation sont disponibles dans `support_tools/sql_scripts/`.

## âœ… Tests et validation

### ExÃ©cuter tous les tests
```bash
pytest -v
```

### Tests avec rapport de couverture
```bash
pytest --cov=pipeline --cov-report=html --cov-report=term-missing
```

Le rapport HTML sera gÃ©nÃ©rÃ© dans le dossier `htmlcov/`.

### Validation des donnÃ©es (Pandera)
Les schÃ©mas de validation Pandera sont dÃ©finis dans `pipeline/schemas/schema_pandera.py`.

Ils garantissent l'intÃ©gritÃ© des donnÃ©es lors du pipeline ETL.

## ğŸ“ Workflow de dÃ©veloppement

### Ajouter une nouvelle dÃ©pendance
```bash
pip install package_name
pip freeze > requirements.txt
```

### CrÃ©er une nouvelle page Streamlit
1. CrÃ©er un fichier dans `streamlit_app/pages/`
2. Respecter la convention de nommage : `N_NomPage.py`
3. Importer les composants nÃ©cessaires depuis `streamlit_app/utils/`

### Ajouter une validation Pandera
1. DÃ©finir le schÃ©ma dans `pipeline/schemas/schema_pandera.py`
2. L'utiliser dans `pipeline/main.py` pour valider les donnÃ©es

## ğŸ› DÃ©pannage

### Port dÃ©jÃ  utilisÃ© (8501)
```bash
streamlit run streamlit_app/1_Home.py --server.port 8502
```

### Erreurs de connexion Ã  la base de donnÃ©es
- VÃ©rifier que le fichier `.env` est correctement configurÃ©
- S'assurer que Docker Compose est lancÃ© si vous utilisez un conteneur PostgreSQL
- VÃ©rifier les logs : `docker-compose logs postgres`

### ProblÃ¨mes d'imports
Assurez-vous que l'environnement virtuel est activÃ© et que les dÃ©pendances sont installÃ©es :
```bash
source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸ“š Ressources

- [Documentation Streamlit](https://docs.streamlit.io/)
- [Documentation Pandera](https://pandera.readthedocs.io/)
- [Documentation PostgreSQL](https://www.postgresql.org/docs/)
- [Documentation Docker Compose](https://docs.docker.com/compose/)

## ğŸ‘¥ Contribution

Avant de contribuer :
1. CrÃ©er une branche pour votre feature
2. Ã‰crire des tests pour toute nouvelle fonctionnalitÃ©
3. Passer les tests et respecter la couverture de code
4. Soumettre une pull request

## ğŸ“„ Licence

[Ã€ complÃ©ter selon votre licence]

---
**DerniÃ¨re mise Ã  jour** : 15 janvier 2026